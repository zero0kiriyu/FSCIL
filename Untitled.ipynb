{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torchtext.data import get_tokenizer\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/envs/science/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "window_size=31\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LABEL = data.Field(sequential=False,use_vocab=False,is_target=True)\n",
    "SESSION = data.Field(sequential=False,use_vocab=False,is_target=False)\n",
    "TEXT = data.Field(fix_length=window_size,lower=True,preprocessing=lambda x:x[0].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/envs/science/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n/usr/local/anaconda3/envs/science/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_large,test,train_fewshot =data.TabularDataset.splits(path='./',\n",
    "    train='train_large.csv',\n",
    "    test='train_fewshot.csv',\n",
    "    validation='test.csv',\n",
    "    format='csv',fields=[('text', TEXT), ('label', LABEL),('session',SESSION)],\n",
    "    skip_header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_large,train_fewshot,test, vectors=GloVe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings=TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/anaconda3/envs/science/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "LARGE_BATCH_SIZE = 128\n",
    "train_large_iterator = data.BucketIterator(\n",
    "    train_large, \n",
    "    batch_size = LARGE_BATCH_SIZE, \n",
    "    sort=False,\n",
    "    device = DEVICE)\n",
    "\n",
    "FEWSHOT_BATCH_SIZE = 5\n",
    "train_fewshot_iterator,test_iterator = data.BucketIterator.splits(\n",
    "    (train_fewshot,test), \n",
    "    batch_size = FEWSHOT_BATCH_SIZE, \n",
    "    sort=True,\n",
    "    sort_key=lambda x:int(x.session),\n",
    "    device = DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0) # (batch_size,seq_len,embed,size)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Block(nn.Module):\n",
    "    def __init__(self,window_size,out_channel,kernel_size,embedding_dim,drop_out):\n",
    "        super(CNN_Block,self).__init__()\n",
    "        self.cnn = nn.Conv2d(in_channels=1,out_channels=out_channel,kernel_size=(kernel_size,embedding_dim),padding=(kernel_size-1,0))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "        self.maxpool1d = nn.MaxPool1d(window_size+kernel_size-1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        cnn_out = self.cnn(x).squeeze(dim=-1)\n",
    "        relu_out = self.relu(cnn_out)\n",
    "        drop_out_result = self.drop_out(relu_out)\n",
    "        return self.maxpool1d(drop_out_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMCNN(nn.Module):\n",
    "    def __init__(self,window_sizes,kernel_sizes,out_channel,pretrained_embeddings,drop_out,total_classes):\n",
    "        super(DMCNN, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.position_encoding = PositionalEncoding(d_model = pretrained_embeddings.size(1),max_len=window_size)\n",
    "        \n",
    "        self.cnn_blocks = nn.ModuleList([\n",
    "            CNN_Block(window_size,\n",
    "                      out_channel,\n",
    "                      kernel_size,\n",
    "                      pretrained_embeddings.size(1),\n",
    "                      drop_out) for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "        self.linear = nn.Linear(in_features=out_channel*len(kernel_sizes)+pretrained_embeddings.size(1),out_features=total_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        word_embedding = self.word_embedding(x) # (batch_size,win_size,embed_size)\n",
    "        position_encoding = self.position_encoding(word_embedding) # (batch_size,win_size,embed_size)\n",
    "        we_pe = position_encoding.unsqueeze(dim=1) # (batch_size,1,win_size,embed_size)\n",
    "        cnn_outs = []\n",
    "        for cnn_block in self.cnn_blocks:\n",
    "            cnn_outs += [cnn_block(we_pe)]\n",
    "        total_cnn_outs = torch.cat(cnn_outs,dim=2) # (batch_size,out_channel,1)\n",
    "        total_cnn_outs = total_cnn_outs.view(total_cnn_outs.size(0),-1) #(batch_size,out_channel * kernel number)\n",
    "        \n",
    "        #下面是论文中没有提到的trick，把cnn输出和前半段的embedding vector和在一起进linear层\n",
    "        total_cnn_outs = torch.cat((total_cnn_outs, word_embedding[:, int((window_size-1)/2)]), dim=-1)\n",
    "        \n",
    "        #print(total_cnn_outs.shape)\n",
    "        drop_out_result = self.drop_out(total_cnn_outs)\n",
    "        y = self.linear(drop_out_result)        \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSCIL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FSCIL,self).__init__()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd04e565a553f81b2e329ec653083fe794b4c021084f2f644610a3d35202f2aa57b",
   "display_name": "Python 3.8.8 64-bit ('science': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}